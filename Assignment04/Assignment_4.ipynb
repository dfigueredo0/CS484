{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dde1f34-a2a0-4b89-be02-5c371c75a4c8",
   "metadata": {},
   "source": [
    "## Assignment 4 (50 marks)\n",
    "#### =====================================================================================================\n",
    "### Deadline: 11/02 11:59 pm\n",
    "#### ====================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bc6f7-a547-4578-b99f-0d8bf3b3a670",
   "metadata": {},
   "source": [
    "### Problem 1: PCA (20 marks)\n",
    "\n",
    "`lab04_dataset_1.csv` contains 205 observations on various vehicles. This is an unsupervised training data. You will use the entire dataset for `PCA`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b2c7a-2e37-4eee-875e-134175fc5a7a",
   "metadata": {},
   "source": [
    "### 1.a (2 marks)\n",
    "\n",
    "For the 14 input features, drop any rows with missing numerical values and output the new length of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88ddaf63-9a31-43a2-aba0-6f601d4e8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.replace('?', pd.NA, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_data = process_data(\"lab04_dataset_1.csv\")\n",
    "    print(cleaned_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de9d8f3a-b1d4-434f-a4bc-bff0deb319c2",
   "metadata": {},
   "source": [
    "### 1.b (6 marks)\n",
    "\n",
    "Using the sklearn's [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) method, compute all the principal components (PCs) of the normalized dataset. All the PCs capture a fraction of the total variance, output all the variances captured by all the PCs. Write a code snippet that checks all the PCs and selects the `top k PCs` whose total variance captured is atleast `90%`. What did `k` come out to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c775e-cabc-488d-8f06-21bdc502d7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [9.95403016e-01 3.54407926e-03 1.04171364e-03 7.69317897e-06\n",
      " 2.34489947e-06 5.87928616e-07 3.22679536e-07 9.49081391e-08\n",
      " 8.01366634e-08 4.02252614e-08 1.39260159e-08 1.09727181e-08\n",
      " 1.24793686e-09]\n",
      "Number of components selected: 13\n",
      "feature names: ['pca0' 'pca1' 'pca2' 'pca3' 'pca4' 'pca5' 'pca6' 'pca7' 'pca8' 'pca9'\n",
      " 'pca10' 'pca11' 'pca12']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "def pca():\n",
    "    df = process_data(\"lab04_dataset_1.csv\")\n",
    "    pca = PCA(n_components=13)\n",
    "    pca.fit(df)\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Number of components selected: {pca.n_components_}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pca()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28c0ef2d-48ba-454d-9804-6218a54d1cf2",
   "metadata": {},
   "source": [
    "### 1.c (3 marks)\n",
    "\n",
    "Using the `top k PCs`, apply `dimensionality reduction` on the normalized dataset to generate and display the transformed dataset which should now have only `k columns`. Display the output as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a335c8-7efa-41a7-9e11-36be602031ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f9fd201-2112-47b6-8470-6d573db2b731",
   "metadata": {},
   "source": [
    "### 1.d (3 marks)\n",
    "\n",
    "We learned in class that we can also obtain the PCs using a matrix decomposition technique called `SVD:` $X=U\\Lambda V$. Use `SVD` on the original normalized dataset to obtain the 3 decomposed matrices and output them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669b8d6-daff-4818-8f9d-0f8cc6ca9de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e097ba50-4ca9-4bbc-b022-5adc4e3623c0",
   "metadata": {},
   "source": [
    "### 1.e (3 marks)\n",
    "\n",
    "Generate the `k` (obtained from `1.b`) largest eigenvalues from the decomposed matrices obtained from the `SVD`. Remember, eigenvalue $\\lambda=\\Lambda^2/n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31baea-9deb-4870-aa0f-d2a7f4ce8368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a33bd7dd-7d06-4b12-bbae-326aa2579f42",
   "metadata": {},
   "source": [
    "### 1.f (3 marks)\n",
    "\n",
    "Generate the projections of the normalized dataset using the `first k PCs` obtained from the `SVD` and display it inside a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11cf48-445f-40ba-9fd9-c323d127a9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd890728-f061-4cf9-84dc-7b6169dc4ce6",
   "metadata": {},
   "source": [
    "### Problem 2: Clustering (30 marks)\n",
    "\n",
    "`lab04_dataset_2.csv` contains 239 observations with two input features *x1* and *x2*.\n",
    "\n",
    "`lab04_dataset_3.csv` contains 1440 observations with two input features *x1* and *x2*.\n",
    "\n",
    "For this task, you will perform various clustering-related operations using sklearn's [`clustering`](https://scikit-learn.org/stable/modules/clustering.html) module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f4655-b381-4e4d-bbcb-e5ae21b28c32",
   "metadata": {},
   "source": [
    "### 2.a (6 marks)\n",
    "\n",
    "Using `lab04_dataset_2.csv`, apply sklearn's `KMeans` algorithm on the two-dimensional data and output the resulting clusters using a scatterplot. You will apply `KMeans` over several clusters ranging from cluster-count `K = 2 to 6`. Make sure for every iteration of different cluster-count, your scatterplot should use `K colors` to clearly distinguish the data points belonging in their respective `K clusters`. Also, compute the `Silhouette score` for each of those `K clusters` and plot that score against `K`. Label the plot axes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e6ae6-8c23-4558-af9f-79e6fa117747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "326232b0-bc8c-4260-8b0c-6cf98714b639",
   "metadata": {},
   "source": [
    "### 2.b (6 marks)\n",
    "\n",
    "Repeat `2.a` but instead use sklearn's `GaussianMixture` model for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912de64-ae18-4d04-be5b-2f3f5019add8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83fc22cd-df60-4728-a254-431dfbd9bc66",
   "metadata": {},
   "source": [
    "### 2.c (6 marks)\n",
    "\n",
    "Using `lab04_dataset_3.csv`, apply sklearn's `AgglomerativeClustering` on the two-dimensional data and output the resulting clusters using a scatterplot. You will apply `AgglomerativeClustering` over several clusters ranging from cluster-count `K = 2 to 6`. Make sure your scatterplot uses `K colors` to clearly distinguish the data points belonging in their respective `K clusters`. Also, compute the `Silhouette score` for each of those `K clusters` and plot the `Silhouette score` against `K clusters`. Label the plot axes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87119a39-6efe-4754-b703-bfb4375cf882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a78022c3-00df-4d6a-a5fe-2176618040f6",
   "metadata": {},
   "source": [
    "### 2.d (6 marks)\n",
    "\n",
    "Repeat `2.c` but instead use sklearn's `SpectralClustering` model for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9773e5c-309c-4980-b500-912d179ea125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7d9de7f-1641-4844-afbc-23ce826dec07",
   "metadata": {},
   "source": [
    "### 2.e (6 marks)\n",
    "\n",
    "The dataset `lab04_dataset_3.csv` generates 4 concentric rings, so ideally we would want 4 clusters representing the 4 concentric rings. Did the clustering attempts in `2.c` and `2.d` lead to 4 concentric ring clusters. Explore some other sklearn clustering algorithms to see which one can actually produce 4 clusters corresponding with the 4 concentric rings and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce34217-44d7-40ed-bdd9-5734b5023105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
