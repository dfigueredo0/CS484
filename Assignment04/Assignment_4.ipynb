{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dde1f34-a2a0-4b89-be02-5c371c75a4c8",
   "metadata": {},
   "source": [
    "## Assignment 4 (50 marks)\n",
    "#### =====================================================================================================\n",
    "### Deadline: 11/02 11:59 pm\n",
    "#### ====================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bc6f7-a547-4578-b99f-0d8bf3b3a670",
   "metadata": {},
   "source": [
    "### Problem 1: PCA (20 marks)\n",
    "\n",
    "`lab04_dataset_1.csv` contains 205 observations on various vehicles. This is an unsupervised training data. You will use the entire dataset for `PCA`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b2c7a-2e37-4eee-875e-134175fc5a7a",
   "metadata": {},
   "source": [
    "### 1.a (2 marks)\n",
    "\n",
    "For the 14 input features, drop any rows with missing numerical values and output the new length of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88ddaf63-9a31-43a2-aba0-6f601d4e8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.replace('?', pd.NA, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop(columns='price')\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_data = process_data(\"lab04_dataset_1.csv\")\n",
    "    print(cleaned_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de9d8f3a-b1d4-434f-a4bc-bff0deb319c2",
   "metadata": {},
   "source": [
    "### 1.b (6 marks)\n",
    "\n",
    "Using the sklearn's [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) method, compute all the principal components (PCs) of the normalized dataset. All the PCs capture a fraction of the total variance, output all the variances captured by all the PCs. Write a code snippet that checks all the PCs and selects the `top k PCs` whose total variance captured is atleast `90%`. What did `k` come out to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa7c775e-cabc-488d-8f06-21bdc502d7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio per PC:\n",
      "PC01: 0.537987\n",
      "PC02: 0.162802\n",
      "PC03: 0.086867\n",
      "PC04: 0.064973\n",
      "PC05: 0.043496\n",
      "PC06: 0.029693\n",
      "PC07: 0.022900\n",
      "PC08: 0.019296\n",
      "PC09: 0.008594\n",
      "PC10: 0.007900\n",
      "PC11: 0.005828\n",
      "PC12: 0.004587\n",
      "PC13: 0.003671\n",
      "PC14: 0.001407\n",
      "\n",
      "Cumulative variance at k=6: 0.9258\n",
      "\n",
      "k = 6 PCs to reach at least 90% total variance\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "def run_pca(path=\"lab04_dataset_1.csv\", threshold=0.9, verbose=True):\n",
    "    X = process_data(path)\n",
    "    Xn = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=None)\n",
    "    pca.fit(Xn)\n",
    "    ratios = pca.explained_variance_ratio_\n",
    "    cumalative_sum = np.cumsum(ratios)\n",
    "    k = np.searchsorted(cumalative_sum, threshold) + 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Explained variance ratio per PC:\")\n",
    "        for i, r in enumerate(ratios, 1):\n",
    "            print(f\"PC{i:02d}: {r:.6f}\")\n",
    "        print(f\"\\nCumulative variance at k={k}: {cumalative_sum[k-1]:.4f}\")   \n",
    "        print(f\"\\nk = {k} PCs to reach at least 90% total variance\") \n",
    "    return Xn, k\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run_pca()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28c0ef2d-48ba-454d-9804-6218a54d1cf2",
   "metadata": {},
   "source": [
    "### 1.c (3 marks)\n",
    "\n",
    "Using the `top k PCs`, apply `dimensionality reduction` on the normalized dataset to generate and display the transformed dataset which should now have only `k columns`. Display the output as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1a335c8-7efa-41a7-9e11-36be602031ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2       PC3       PC4       PC5       PC6\n",
      "0 -0.612999 -2.164573 -0.298752 -2.436486  0.194563 -0.116741\n",
      "1 -0.493886 -2.190732 -0.248341 -2.476630  0.331332 -0.157826\n",
      "2  0.443900 -1.365449  1.449443  0.626999  0.359939 -2.002387\n",
      "3 -0.178982 -0.256392  0.066248  1.153280  0.277470  0.118314\n",
      "4  1.269804 -1.167075  0.018756  1.204362  0.048029 -0.332126\n"
     ]
    }
   ],
   "source": [
    "def dim_reduction(path=\"lab04_dataset_1.csv\"):\n",
    "    Xn, k = run_pca(path, 0.9, False)\n",
    "    \n",
    "    X_reduced = PCA(n_components=k).fit(Xn).transform(Xn)[:, :k]\n",
    "    reduced_df = pd.DataFrame(X_reduced, columns=[f\"PC{i+1}\" for i in range(k)])\n",
    "    print(reduced_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dim_reduction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9fd201-2112-47b6-8470-6d573db2b731",
   "metadata": {},
   "source": [
    "### 1.d (3 marks)\n",
    "\n",
    "We learned in class that we can also obtain the PCs using a matrix decomposition technique called `SVD:` $X=U\\Lambda V$. Use `SVD` on the original normalized dataset to obtain the 3 decomposed matrices and output them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4669b8d6-daff-4818-8f9d-0f8cc6ca9de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U          u1        u2        u3        u4        u5        u6        u7  \\\n",
      "0 -0.015995  0.102674  0.019400  0.182944 -0.017855  0.012966  0.063217   \n",
      "1 -0.012887  0.103915  0.016127  0.185958 -0.030406  0.017530  0.047363   \n",
      "2  0.011583  0.064769 -0.094122 -0.047078 -0.033031  0.222402 -0.091994   \n",
      "3 -0.004670  0.012162 -0.004302 -0.086594 -0.025463 -0.013141  0.022138   \n",
      "4  0.033134  0.055359 -0.001218 -0.090430 -0.004408  0.036889 -0.016175   \n",
      "\n",
      "         u8        u9       u10       u11       u12       u13       u14  \n",
      "0  0.134816  0.031601  0.044295 -0.111749  0.101114 -0.048439  0.008628  \n",
      "1  0.121088  0.010908  0.079402 -0.154791  0.079009 -0.062272  0.021627  \n",
      "2  0.183118  0.041210 -0.102957  0.007346 -0.098261 -0.027142 -0.067653  \n",
      "3  0.012599 -0.033787  0.019578 -0.072130 -0.066190  0.082089  0.012842  \n",
      "4  0.097954 -0.052015  0.089414 -0.012797  0.044259 -0.032666  0.035416   \n",
      "\n",
      "Sigma           s1         s2         s3         s4         s5   s6   s7   s8   s9  \\\n",
      "0  38.323675   0.000000   0.000000   0.000000   0.000000  0.0  0.0  0.0  0.0   \n",
      "1   0.000000  21.081984   0.000000   0.000000   0.000000  0.0  0.0  0.0  0.0   \n",
      "2   0.000000   0.000000  15.399544   0.000000   0.000000  0.0  0.0  0.0  0.0   \n",
      "3   0.000000   0.000000   0.000000  13.318219   0.000000  0.0  0.0  0.0  0.0   \n",
      "4   0.000000   0.000000   0.000000   0.000000  10.896959  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   s10  s11  s12  s13  s14  \n",
      "0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "V             0         1         2         3         4         5         6   \\\n",
      "PC01  0.288205  0.328462  0.323878  0.110933  0.351887  0.321954  0.259218   \n",
      "PC02 -0.291823 -0.163362 -0.125795 -0.399506 -0.061691  0.080699  0.003813   \n",
      "PC03  0.129942  0.127445 -0.051597  0.476168 -0.054392 -0.250068  0.166390   \n",
      "PC04 -0.240405 -0.147081 -0.093279 -0.391560  0.015319  0.182072  0.393772   \n",
      "PC05  0.040390  0.007462 -0.128867  0.001618 -0.057462 -0.083907  0.317119   \n",
      "\n",
      "            7         8         9         10        11        12        13  \n",
      "PC01  0.052470  0.014561  0.297893 -0.081171 -0.309090 -0.319202  0.318518  \n",
      "PC02 -0.114614 -0.520135  0.301938  0.446148 -0.272495 -0.222155  0.069951  \n",
      "PC03 -0.704354 -0.283579 -0.141371  0.058140 -0.114011 -0.115015 -0.134802  \n",
      "PC04 -0.479062  0.168418  0.085794 -0.527444  0.085718  0.088415  0.107348  \n",
      "PC05  0.433100 -0.498544 -0.131013 -0.489851 -0.155028 -0.140578 -0.365732  \n"
     ]
    }
   ],
   "source": [
    "def run_svd(path=\"lab04_dataset_1.csv\"):\n",
    "    X = process_data(path)\n",
    "    Xn = StandardScaler().fit_transform(X)\n",
    "\n",
    "    U, s, V = np.linalg.svd(Xn, full_matrices=False)\n",
    "    Sigma = np.diag(s)\n",
    "    \n",
    "    U_df = pd.DataFrame(U, X.index, columns=[f\"u{i+1}\" for i in range(U.shape[1])])\n",
    "    Sigma_df = pd.DataFrame(Sigma, columns=[f\"s{i+1}\" for i in range(Sigma.shape[1])])\n",
    "    V_df = pd.DataFrame(V, index=[f\"PC{i+1:02d}\" for i in range(V.shape[0])])\n",
    "    \n",
    "    print(\"U\", U_df.head(), \"\\n\")\n",
    "    print(\"Sigma\", Sigma_df.head(), \"\\n\")\n",
    "    print(\"V\", V_df.head())\n",
    "    \n",
    "    return U_df, Sigma_df, V_df\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run_svd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097ba50-4ca9-4bbc-b022-5adc4e3623c0",
   "metadata": {},
   "source": [
    "### 1.e (3 marks)\n",
    "\n",
    "Generate the `k` (obtained from `1.b`) largest eigenvalues from the decomposed matrices obtained from the `SVD`. Remember, eigenvalue $\\lambda=\\Lambda^2/n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c31baea-9deb-4870-aa0f-d2a7f4ce8368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.53181553 2.27923094 1.21613308 0.90961519 0.60894217 0.4157043 ]\n"
     ]
    }
   ],
   "source": [
    "def largest_eigen(path=\"lab04_dataset_1.csv\"):\n",
    "    Xn, k = run_pca(path, verbose=False)\n",
    "    \n",
    "    U, s, V = np.linalg.svd(Xn, full_matrices=False)\n",
    "\n",
    "    eigvals = (s**2) / Xn.shape[0]\n",
    "    return eigvals[:k]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(largest_eigen())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33bd7dd-7d06-4b12-bbae-326aa2579f42",
   "metadata": {},
   "source": [
    "### 1.f (3 marks)\n",
    "\n",
    "Generate the projections of the normalized dataset using the `first k PCs` obtained from the `SVD` and display it inside a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11cf48-445f-40ba-9fd9-c323d127a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection():\n",
    "    Xn, k = run_pca(verbose=False)\n",
    "    U, s, v = np.linalg.svd(Xn, full_matrices=False)\n",
    "    \n",
    "    Z = U[:, :k] * s[:k]\n",
    "    proj_df = pd.DataFrame(Z, columns=[f\"PC{i+1}\" for i in range(k)])\n",
    "    print(proj_df.head())\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    projection()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd890728-f061-4cf9-84dc-7b6169dc4ce6",
   "metadata": {},
   "source": [
    "### Problem 2: Clustering (30 marks)\n",
    "\n",
    "`lab04_dataset_2.csv` contains 239 observations with two input features *x1* and *x2*.\n",
    "\n",
    "`lab04_dataset_3.csv` contains 1440 observations with two input features *x1* and *x2*.\n",
    "\n",
    "For this task, you will perform various clustering-related operations using sklearn's [`clustering`](https://scikit-learn.org/stable/modules/clustering.html) module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f4655-b381-4e4d-bbcb-e5ae21b28c32",
   "metadata": {},
   "source": [
    "### 2.a (6 marks)\n",
    "\n",
    "Using `lab04_dataset_2.csv`, apply sklearn's `KMeans` algorithm on the two-dimensional data and output the resulting clusters using a scatterplot. You will apply `KMeans` over several clusters ranging from cluster-count `K = 2 to 6`. Make sure for every iteration of different cluster-count, your scatterplot should use `K colors` to clearly distinguish the data points belonging in their respective `K clusters`. Also, compute the `Silhouette score` for each of those `K clusters` and plot that score against `K`. Label the plot axes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e6ae6-8c23-4558-af9f-79e6fa117747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "326232b0-bc8c-4260-8b0c-6cf98714b639",
   "metadata": {},
   "source": [
    "### 2.b (6 marks)\n",
    "\n",
    "Repeat `2.a` but instead use sklearn's `GaussianMixture` model for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912de64-ae18-4d04-be5b-2f3f5019add8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83fc22cd-df60-4728-a254-431dfbd9bc66",
   "metadata": {},
   "source": [
    "### 2.c (6 marks)\n",
    "\n",
    "Using `lab04_dataset_3.csv`, apply sklearn's `AgglomerativeClustering` on the two-dimensional data and output the resulting clusters using a scatterplot. You will apply `AgglomerativeClustering` over several clusters ranging from cluster-count `K = 2 to 6`. Make sure your scatterplot uses `K colors` to clearly distinguish the data points belonging in their respective `K clusters`. Also, compute the `Silhouette score` for each of those `K clusters` and plot the `Silhouette score` against `K clusters`. Label the plot axes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87119a39-6efe-4754-b703-bfb4375cf882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a78022c3-00df-4d6a-a5fe-2176618040f6",
   "metadata": {},
   "source": [
    "### 2.d (6 marks)\n",
    "\n",
    "Repeat `2.c` but instead use sklearn's `SpectralClustering` model for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9773e5c-309c-4980-b500-912d179ea125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7d9de7f-1641-4844-afbc-23ce826dec07",
   "metadata": {},
   "source": [
    "### 2.e (6 marks)\n",
    "\n",
    "The dataset `lab04_dataset_3.csv` generates 4 concentric rings, so ideally we would want 4 clusters representing the 4 concentric rings. Did the clustering attempts in `2.c` and `2.d` lead to 4 concentric ring clusters. Explore some other sklearn clustering algorithms to see which one can actually produce 4 clusters corresponding with the 4 concentric rings and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce34217-44d7-40ed-bdd9-5734b5023105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
